{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e713451d-8aba-43eb-8486-009371d644b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Interaction_Description</th>\n",
       "      <th>Simplified_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trioxsalen may increase the photosensitizing a...</td>\n",
       "      <td>When trioxsalen and verteporfin are used toget...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aminolevulinic acid may increase the photosens...</td>\n",
       "      <td>Aminolevulinic acid can make your skin more se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Titanium dioxide may increase the photosensiti...</td>\n",
       "      <td>Titanium dioxide can make your skin more sensi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The risk or severity of adverse effects can be...</td>\n",
       "      <td>Using Paclitaxel with Verteporfin can increase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The risk or severity of adverse effects can be...</td>\n",
       "      <td>Combining Docetaxel with Verteporfin can raise...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Interaction_Description  \\\n",
       "0  Trioxsalen may increase the photosensitizing a...   \n",
       "1  Aminolevulinic acid may increase the photosens...   \n",
       "2  Titanium dioxide may increase the photosensiti...   \n",
       "3  The risk or severity of adverse effects can be...   \n",
       "4  The risk or severity of adverse effects can be...   \n",
       "\n",
       "                              Simplified_Description  \n",
       "0  When trioxsalen and verteporfin are used toget...  \n",
       "1  Aminolevulinic acid can make your skin more se...  \n",
       "2  Titanium dioxide can make your skin more sensi...  \n",
       "3  Using Paclitaxel with Verteporfin can increase...  \n",
       "4  Combining Docetaxel with Verteporfin can raise...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV\n",
    "df = pd.read_csv(\"finetune_data.csv\", encoding=\"ISO-8859-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87cb19c3-6d10-4a02-a9e5-71c5b2b163be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\aana\\envs\\ddi_sim\\lib\\site-packages\\spacy\\language.py:2195: FutureWarning: Possible set union at position 6328\n",
      "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2023-04-23/umls/tfidf_vectors_sparse.npz not found in cache, downloading to C:\\Users\\POWERU~1\\AppData\\Local\\Temp\\tmp8q6g5s51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 492M/492M [06:16<00:00, 1.37MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished download, copying C:\\Users\\POWERU~1\\AppData\\Local\\Temp\\tmp8q6g5s51 to cache at C:\\Users\\power user\\.scispacy\\datasets\\2b79923846fb52e62d686f2db846392575c8eb5b732d9d26cd3ca9378c622d40.87bd52d0f0ee055c1e455ef54ba45149d188552f07991b765da256a1b512ca0b.tfidf_vectors_sparse.npz\n",
      "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2023-04-23/umls/nmslib_index.bin not found in cache, downloading to C:\\Users\\POWERU~1\\AppData\\Local\\Temp\\tmp1hqtkoj9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 724M/724M [03:55<00:00, 3.22MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished download, copying C:\\Users\\POWERU~1\\AppData\\Local\\Temp\\tmp1hqtkoj9 to cache at C:\\Users\\power user\\.scispacy\\datasets\\7e8e091ec80370b87b1652f461eae9d926e543a403a69c1f0968f71157322c25.6d801a1e14867953e36258b0e19a23723ae84b0abd2a723bdd3574c3e0c873b4.nmslib_index.bin\n",
      "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2023-04-23/umls/tfidf_vectorizer.joblib not found in cache, downloading to C:\\Users\\POWERU~1\\AppData\\Local\\Temp\\tmpbzv88n0q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.32M/1.32M [00:02<00:00, 667kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished download, copying C:\\Users\\POWERU~1\\AppData\\Local\\Temp\\tmpbzv88n0q to cache at C:\\Users\\power user\\.scispacy\\datasets\\37bc06bb7ce30de7251db5f5cbac788998e33b3984410caed2d0083187e01d38.f0994c1b61cc70d0eb96dea4947dddcb37460fb5ae60975013711228c8fe3fba.tfidf_vectorizer.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\aana\\envs\\ddi_sim\\lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.1.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\aana\\envs\\ddi_sim\\lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.1.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2023-04-23/umls/concept_aliases.json not found in cache, downloading to C:\\Users\\POWERU~1\\AppData\\Local\\Temp\\tmpwldntm2o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 264M/264M [00:56<00:00, 4.86MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished download, copying C:\\Users\\POWERU~1\\AppData\\Local\\Temp\\tmpwldntm2o to cache at C:\\Users\\power user\\.scispacy\\datasets\\6238f505f56aca33290aab44097f67dd1b88880e3be6d6dcce65e56e9255b7d4.d7f77b1629001b40f1b1bc951f3a890ff2d516fb8fbae3111b236b31b33d6dcf.concept_aliases.json\n",
      "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/kbs/2023-04-23/umls_2022_ab_cat0129.jsonl not found in cache, downloading to C:\\Users\\POWERU~1\\AppData\\Local\\Temp\\tmpne16j_3h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 628M/628M [01:34<00:00, 6.94MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished download, copying C:\\Users\\POWERU~1\\AppData\\Local\\Temp\\tmpne16j_3h to cache at C:\\Users\\power user\\.scispacy\\datasets\\d5e593bc2d8adeee7754be423cd64f5d331ebf26272074a2575616be55697632.0660f30a60ad00fffd8bbf084a18eb3f462fd192ac5563bf50940fc32a850a3c.umls_2022_ab_cat0129.jsonl\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import scispacy\n",
    "from scispacy.linking import EntityLinker\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_lg\")\n",
    "nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True, \"linker_name\": \"umls\"})\n",
    "linker = nlp.get_pipe(\"scispacy_linker\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb506a2c-5ded-4a00-9b8b-9013414b24b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"./t5_simplifier_model\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"./t5_simplifier_model\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"data/finetune_data.csv\")\n",
    "\n",
    "# Sample evaluation data (you must have this column!)\n",
    "df = df.dropna(subset=[\"Interaction_Description\", \"Simplified_Description\"])\n",
    "sample = df.sample(n=50)  # or len(df) if small\n",
    "\n",
    "smoothie = SmoothingFunction().method4\n",
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "bleu_scores, rouge1_scores, rougeL_scores = [], [], []\n",
    "\n",
    "# Generate & evaluate\n",
    "for i, row in sample.iterrows():\n",
    "    input_text = row[\"Interaction_Description\"]\n",
    "    reference = row[\"Simplified_Description\"]\n",
    "\n",
    "    # Generate from model\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    outputs = model.generate(inputs, max_length=64)\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # BLEU\n",
    "    bleu = sentence_bleu([reference.split()], prediction.split(), smoothing_function=smoothie)\n",
    "    bleu_scores.append(bleu)\n",
    "\n",
    "    # ROUGE\n",
    "    r_scores = rouge.score(reference, prediction)\n",
    "    rouge1_scores.append(r_scores[\"rouge1\"].fmeasure)\n",
    "    rougeL_scores.append(r_scores[\"rougeL\"].fmeasure)\n",
    "\n",
    "# Print average scores\n",
    "print(f\"ðŸ”µ Avg BLEU: {sum(bleu_scores)/len(bleu_scores):.4f}\")\n",
    "print(f\"ðŸ”´ Avg ROUGE-1: {sum(rouge1_scores)/len(rouge1_scores):.4f}\")\n",
    "print(f\"ðŸŸ  Avg ROUGE-L: {sum(rougeL_scores)/len(rougeL_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8144748d-2dd0-4eae-8531-44f132b286fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from simplify_model import Simplifier\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from sacrebleu.metrics import SARI\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load the model\n",
    "simplifier = Simplifier(model_path=\"./t5_simplifier_model\")\n",
    "\n",
    "# Load test data\n",
    "df = pd.read_csv(\"test_data.csv\")  # Columns: Interaction_Description, Simplified_Description\n",
    "\n",
    "# Initialize metrics\n",
    "bleu_scores = []\n",
    "rouge_1_scores = []\n",
    "rouge_l_scores = []\n",
    "sari_scores = []\n",
    "\n",
    "sari = SARI()\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "# Iterate through test set\n",
    "for _, row in df.iterrows():\n",
    "    source = row[\"Interaction_Description\"]\n",
    "    reference = row[\"Simplified_Description\"]\n",
    "    \n",
    "    generated = simplifier.simplify(source)\n",
    "\n",
    "    # Tokenized for BLEU\n",
    "    ref_tokens = [word_tokenize(reference)]\n",
    "    gen_tokens = word_tokenize(generated)\n",
    "\n",
    "    # BLEU\n",
    "    bleu = sentence_bleu(ref_tokens, gen_tokens, smoothing_function=smoothie)\n",
    "    bleu_scores.append(bleu)\n",
    "\n",
    "    # ROUGE\n",
    "    rouge = scorer.score(reference, generated)\n",
    "    rouge_1_scores.append(rouge[\"rouge1\"].fmeasure)\n",
    "    rouge_l_scores.append(rouge[\"rougeL\"].fmeasure)\n",
    "\n",
    "    # SARI\n",
    "    sari_score = sari.corpus_score([generated], [[reference]], [source]).score\n",
    "    sari_scores.append(sari_score)\n",
    "\n",
    "# Compute averages\n",
    "avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "avg_rouge1 = sum(rouge_1_scores) / len(rouge_1_scores)\n",
    "avg_rougeL = sum(rouge_l_scores) / len(rouge_l_scores)\n",
    "avg_sari = sum(sari_scores) / len(sari_scores)\n",
    "\n",
    "# Print results\n",
    "print(\"Performance Metrics:\")\n",
    "print(f\"Average BLEU Score     : {avg_bleu:.4f}\")\n",
    "print(f\"Average ROUGE-1 F1     : {avg_rouge1:.4f}\")\n",
    "print(f\"Average ROUGE-L F1     : {avg_rougeL:.4f}\")\n",
    "print(f\"Average SARI Score     : {avg_sari:.4f}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(['BLEU', 'ROUGE-1', 'ROUGE-L', 'SARI'], \n",
    "        [avg_bleu, avg_rouge1, avg_rougeL, avg_sari], \n",
    "        color=['skyblue', 'lightgreen', 'salmon', 'plum'])\n",
    "plt.title(\"Model Performance Metrics\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ddi_simplifier)",
   "language": "python",
   "name": "ddi_simplifier"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
